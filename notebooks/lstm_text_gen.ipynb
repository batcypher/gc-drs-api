{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_gen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsnfiARzYZ3R"
      },
      "source": [
        "vocab_size = 15165\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ8uO_G5Ycne"
      },
      "source": [
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self,n_hidden=256,n_layers = 4,drop_prob = 0.3,lr = 0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        self.emb_layer = nn.Embedding(vocab_size,200)\n",
        "        self.lstm = nn.LSTM(200,n_hidden,n_layers,dropout = drop_prob,batch_first = True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(n_hidden,vocab_size)\n",
        "    \n",
        "    def forward(self,x,hidden):\n",
        "        embedded = self.emb_layer(x)\n",
        "        lstm_output,hidden = self.lstm(embedded,hidden)\n",
        "        out = self.dropout(lstm_output)\n",
        "        out = out.reshape(-1,self.n_hidden)\n",
        "        out = self.fc(out)\n",
        "        return out,hidden\n",
        "    \n",
        "    def init_hidden(self,batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (torch.cuda.is_available()):\n",
        "            hidden = (weight.new(self.n_layers,batch_size,self.n_hidden).zero_().to(device),\n",
        "                      weight.new(self.n_layers,batch_size,self.n_hidden).zero_().to(device))\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        return hidden"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au_R9rW6Ymxv",
        "outputId": "8e8d1589-d835-4e4e-c235-8d3a3fadf65b"
      },
      "source": [
        "model = torch.load('model.pt',map_location=torch.device('cpu'))\n",
        "print(model.eval())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(15165, 200)\n",
            "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=15165, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WboIP4Pkbi6C"
      },
      "source": [
        "def predict(model, tkn, h=None):\n",
        "         \n",
        "  # tensor inputs\n",
        "  x = np.array([[token2int[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "  \n",
        "  # push to GPU\n",
        "  inputs = inputs.to(device)\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = model(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  p = p.cpu()\n",
        "\n",
        "  p = p.numpy()\n",
        "  p = p.reshape(p.shape[1],)\n",
        "\n",
        "  # get indices of top 3 values\n",
        "  top_n_idx = p.argsort()[-3:][::-1]\n",
        "\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int2token[sampled_token_index], h"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_ee0uEYanvj"
      },
      "source": [
        "def generate_text(model,size,prime = \"once upon\"):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    # batch size is 1\n",
        "    h = model.init_hidden(1)\n",
        "    toks = prime.split()\n",
        "\n",
        "    for t in prime.split():\n",
        "        token,h = predict(model,t,h)\n",
        "    toks.append(token)\n",
        "\n",
        "    for i in range(size-1):\n",
        "        token,h = predict(model,toks[-1],h)\n",
        "        toks.append(token)\n",
        "    return \" \".join(toks)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsggiWo4b59e"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-Gbkdj8cSKe"
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "file = open('/content/plots_text.pickle','rb')\n",
        "movie_plots = pickle.load(file)\n",
        "movie_plots = [re.sub(\"[^a-z]\",\" \",i)for i in movie_plots]\n",
        "# creating integer-to-token mapping\n",
        "int2token = {}\n",
        "count = 0\n",
        "for i in set(\" \".join(movie_plots).split()):\n",
        "    int2token[count] = i\n",
        "    count += 1\n",
        "\n",
        "# creating token-to-integer mapping\n",
        "\n",
        "token2int = {t: i for i,t in int2token.items()}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5CoYGEeubFTq",
        "outputId": "3c865a0b-6f5a-4b2c-d1c3-ab5e96db0741"
      },
      "source": [
        "generate_text(model,15,prime='once i was')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'once i was framed wilde round chickens bitterness transit pitcher notified transit pitcher notified round extent drug while'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfWFrzsBbvUu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}